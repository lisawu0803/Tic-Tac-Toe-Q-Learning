{"cells":[{"cell_type":"markdown","source":["#Tic-Tac-Toe\n","\n","Tic Tac Toe game implemented with Q-learning in Python"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"WULml9444eMs"}},{"cell_type":"markdown","source":["**Imports and Constants**"],"metadata":{"id":"kj_dOmOoPM8F"}},{"cell_type":"code","source":["import pygame\n","import numpy as np\n","import random\n","import pickle"],"metadata":{"id":"DK7HitJAliHK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Game Settings**\n","*   WIDTH, HEIGHT: Dimensions of the game window.\n","*   LINE_WIDTH: Width of the lines drawn for the grid.\n","*   BOARD_ROWS, BOARD_COLS: Tic Tac Toe is 3x3, so both are set to 3.\n","*   SQUARE_SIZE: Size of each cell in the grid."],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"HEEgJF2x4eMu"}},{"cell_type":"code","source":["WIDTH, HEIGHT = 300, 300\n","LINE_WIDTH = 10\n","BOARD_ROWS, BOARD_COLS = 3, 3\n","SQUARE_SIZE = WIDTH // BOARD_COLS\n","\n","WHITE = (255, 255, 255)\n","BLACK = (0, 0, 0)\n","RED = (255, 0, 0)\n","GREEN = (0, 255, 0)"],"metadata":{"id":"mHDTVIx3mtFz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Q-learning Settings**"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"lgFqAVPY4eMx"}},{"cell_type":"code","source":["epsilon = 0.9  # Exploration rate\n","alpha = 0.2    # Learning rate\n","gamma = 0.9    # Discount factor"],"metadata":{"id":"LMq9zdXdm6vP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Game State Constants**\n","\n","Constants representing empty cells, player X, and player O."],"metadata":{"id":"xVxp79Q4nCDl"}},{"cell_type":"code","source":["EMPTY = 0\n","X = 1\n","O = -1"],"metadata":{"id":"n5b4hl8onBU6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TicTacToe Class**\n","\\\n","Initializes the game board as a 3x3 grid of zeros (empty).Sets the current player to X.\n","\n","\n","Key Methods:\n","\\\n","reset: Resets the board and current player.\n","\\\n","is_winner: Checks if the given player has won by checking rows, columns, and diagonals.\n","\\\n","is_full: Checks if the board is full (no empty cells).\n","\\\n","available_actions: Returns a list of coordinates for empty cells.\n","\\\n","make_move: Updates the board with the current player's move and switches to the other player."],"metadata":{"id":"6xjhJnS6nKUy"}},{"cell_type":"code","source":["class TicTacToe:\n","    def __init__(self):\n","        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))  # Initialize the game board\n","        self.current_player = X  # Set the current player to X\n","\n","    def reset(self):\n","        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))  # Reset the board\n","        self.current_player = X  # Reset the current player to X\n","\n","    def is_winner(self, player):\n","        # Check rows, columns, and diagonals for a win\n","        for row in range(BOARD_ROWS):\n","            if np.all(self.board[row, :] == player):\n","                return True\n","        for col in range(BOARD_COLS):\n","            if np.all(self.board[:, col] == player):\n","                return True\n","        if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n","            return True\n","        return False\n","\n","    def is_full(self):\n","        return np.all(self.board != EMPTY)  # Check if the board is full\n","\n","    def available_actions(self):\n","        # Return a list of available actions (empty spaces)\n","        return [(r, c) for r in range(BOARD_ROWS) for c in range(BOARD_COLS) if self.board[r, c] == EMPTY]\n","\n","    def make_move(self, row, col):\n","        # Make a move if the space is empty\n","        if self.board[row, col] == EMPTY:\n","            self.board[row, col] = self.current_player\n","            self.current_player = O if self.current_player == X else X\n","            return True\n","        return False"],"metadata":{"id":"TKu8ubNKnZcx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**QLearningAgent Class**\n","\\\n","Initializes an empty Q-table to store state-action values.\n","\n","Key Methods\n","\\\n","save_q_table: Saves the Q-table to a file using pickle.\n","\\\n","load_q_table: Loads the Q-table from a file, initializing it if the file does not exist.\n","\\\n","get_state_key: Converts the board state to a string key for the Q-table.\n","\\\n","choose_action: Chooses an action using the epsilon-greedy strategy:\n","*   With probability epsilon, it explores by selecting a random action.\n","*   Otherwise, it selects the action with the highest Q-value from the Q-table.\n","\n","learn: Updates the Q-value for the taken action using the Q-learning formula based on the received reward and estimated future rewards."],"metadata":{"id":"whlzBe56n9Xk"}},{"cell_type":"code","source":["class QLearningAgent:\n","    def __init__(self):\n","        self.q_table = {}  # Initialize the Q-table\n","\n","    def save_q_table(self, filename='q_table.pkl'):\n","        # Save the Q-table to a file\n","        try:\n","            with open(filename, 'wb') as f:\n","                pickle.dump(self.q_table, f)  # Save Q-table to file\n","            print(f\"Q-table saved to {filename}\")  # Confirm successful save\n","        except Exception as e:\n","            print(f\"Error saving Q-table: {e}\")\n","\n","    def load_q_table(self, filename='q_table.pkl'):\n","        # Load the Q-table from a file\n","        try:\n","            with open(filename, 'rb') as f:\n","                self.q_table = pickle.load(f)  # Load Q-table\n","                print(\"Q-table loaded successfully.\")\n","        except FileNotFoundError:\n","            print(\"Q-table file not found, starting fresh.\")\n","            self.q_table = {}  # Initialize to empty if no Q-table file exists\n","        except Exception as e:\n","            print(f\"Error loading Q-table: {e}\")\n","\n","    def get_state_key(self, board):\n","        return str(board.reshape(9))  # Convert board state to a key\n","\n","    def choose_action(self, state, available_actions):\n","        # Choose an action based on epsilon-greedy strategy\n","        if random.uniform(0, 1) < epsilon:\n","            return random.choice(available_actions)  # Explore\n","        else:\n","            q_values = [self.q_table.get((state, (r, c)), 0) for r, c in available_actions]\n","            max_q = max(q_values)  # Get maximum Q-value\n","            return available_actions[q_values.index(max_q)]  # Choose the best action\n","\n","    def learn(self, state, action, reward, next_state, available_actions):\n","        state_key = self.get_state_key(state)\n","        next_state_key = self.get_state_key(next_state)\n","        current_q = self.q_table.get((state_key, action), 0)\n","\n","        # Get maximum Q-value of the next state\n","        future_q = max([self.q_table.get((next_state_key, a), 0) for a in available_actions], default=0)\n","\n","        # Update the Q-value for the state-action pair\n","        self.q_table[(state_key, action)] = current_q + alpha * (reward + gamma * future_q - current_q)\n","\n","        print(f\"Updated Q-value for {state_key}, {action}: {self.q_table[(state_key, action)]}\")"],"metadata":{"id":"wC_l2LwUo6BS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Drawing the Board**\n","\n","This function draws the game grid and the current state of the board, rendering Xs and Os as per the current game state."],"metadata":{"id":"Wl-xFwuvpFOG"}},{"cell_type":"code","source":["def draw_board(board):\n","    # Draw grid lines\n","    for r in range(1, BOARD_ROWS):\n","        pygame.draw.line(screen, BLACK, (0, r * SQUARE_SIZE), (WIDTH, r * SQUARE_SIZE), LINE_WIDTH)\n","    for c in range(1, BOARD_COLS):\n","        pygame.draw.line(screen, BLACK, (c * SQUARE_SIZE, 0), (c * SQUARE_SIZE, HEIGHT), LINE_WIDTH)\n","\n","    # Draw X and O\n","    for r in range(BOARD_ROWS):\n","        for c in range(BOARD_COLS):\n","            if board[r, c] == X:\n","                pygame.draw.line(screen, GREEN, (c * SQUARE_SIZE + 10, r * SQUARE_SIZE + 10), (c * SQUARE_SIZE + SQUARE_SIZE - 10, r * SQUARE_SIZE + SQUARE_SIZE - 10), LINE_WIDTH)\n","                pygame.draw.line(screen, GREEN, (c * SQUARE_SIZE + SQUARE_SIZE - 10, r * SQUARE_SIZE + 10), (c * SQUARE_SIZE + 10, r * SQUARE_SIZE + SQUARE_SIZE - 10), LINE_WIDTH)\n","            elif board[r, c] == O:\n","                pygame.draw.circle(screen, RED, (c * SQUARE_SIZE + SQUARE_SIZE // 2, r * SQUARE_SIZE + SQUARE_SIZE // 2), SQUARE_SIZE // 3)"],"metadata":{"id":"afFsWdH8pNCn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Main Game Loop**\n","\n","Game Initialization: Creates instances of TicTacToe and QLearningAgent, and loads the Q-table.\n","\n","Game Loop: Continues while the game is running:\n","*   Clears the screen and draws the board.\n","*   Checks for a win or draw condition.\n","*   If the game continues, the AI makes a move based on the current state and updates the Q-table.\n","\n","End of Game: Saves the Q-table before quitting."],"metadata":{"id":"d9HfcV55pSTq"}},{"cell_type":"code","source":["# Initialize Pygame\n","pygame.init()\n","screen = pygame.display.set_mode((WIDTH, HEIGHT))\n","pygame.display.set_caption('Tic Tac Toe with Q-Learning')\n","\n","def main():\n","    game = TicTacToe()  # Create a TicTacToe instance\n","    agent = QLearningAgent()  # Create a QLearningAgent instance\n","\n","    # Load Q-table\n","    agent.load_q_table()\n","\n","    clock = pygame.time.Clock()\n","    running = True\n","\n","    while running:\n","        screen.fill(WHITE)  # Clear the screen\n","        draw_board(game.board)  # Draw the game board\n","\n","        # Check if the game has ended and print results\n","        if game.is_winner(X):\n","            print(\"X wins!\")\n","            reward = -1  # Punish AI if X wins\n","            game.reset()  # Reset the game\n","            agent.save_q_table()  # Save Q-table\n","            continue\n","        elif game.is_winner(O):\n","            print(\"O wins!\")\n","            reward = 1  # Reward AI if O wins\n","            game.reset()  # Reset the game\n","            agent.save_q_table()  # Save Q-table\n","            continue\n","        elif game.is_full():\n","            print(\"It's a draw!\")\n","            reward = 0  # No reward for a draw\n","            game.reset()  # Reset the game\n","            agent.save_q_table()  # Save Q-table\n","            continue\n","        else:\n","            reward = -0.1  # Small penalty for continuing the game\n","\n","        # If the game continues, AI makes a choice\n","        state = game.board.copy()\n","        available_actions = game.available_actions()\n","\n","        # If there are available actions, AI performs action\n","        if available_actions:\n","            action = agent.choose_action(agent.get_state_key(state), available_actions)\n","            game.make_move(action[0], action[1])\n","\n","            # Update Q-table at the end of each round\n","            next_state = game.board.copy()\n","            agent.learn(state, action, reward, next_state, available_actions)\n","\n","        pygame.display.flip()  # Update the display\n","        clock.tick(1)  # Control the game frame rate\n","\n","    # Save Q-table after the game ends\n","    agent.save_q_table()\n","\n","    pygame.quit()  # Quit Pygame"],"metadata":{"id":"Jy_13eM0pzJn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exception Handling**\n","\n","Handles graceful termination and saves the Q-table if an error occurs or if the game is interrupted."],"metadata":{"id":"LnMrkVbmp0HG"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    try:\n","        main()  # Run the main function\n","    except KeyboardInterrupt:\n","        print(\"Training interrupted. Saving Q-table...\")\n","        agent.save_q_table()  # Save on interrupt\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        agent.save_q_table()  # Save on exception"],"metadata":{"id":"3jFSc7EOp66C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# AI self-training code\n","\n","This code file contains the training part of the Q-learning algorithm for the Tic Tac Toe game. It simulates an agent playing against itself for 3000 rounds, optimizing its strategy by continuously updating the Q value. The code includes the following elements:\n","\n","*   Initialize the Q table and related parameters (such as epsilon, alpha, gamma).\n","*   The main loop of the game, where the agent selects actions based on the current strategy and updates the Q value.\n","*   The results of each training round are recorded for subsequent analysis."],"metadata":{"id":"EmTY7Z9EOiNa"}},{"cell_type":"code","source":["import pygame\n","import numpy as np\n","import random\n","import pickle\n","\n","# Game settings\n","WIDTH, HEIGHT = 300, 300\n","LINE_WIDTH = 10\n","BOARD_ROWS, BOARD_COLS = 3, 3\n","SQUARE_SIZE = WIDTH // BOARD_COLS\n","\n","# Colors\n","WHITE = (255, 255, 255)\n","BLACK = (0, 0, 0)\n","RED = (255, 0, 0)\n","GREEN = (0, 255, 0)\n","\n","# Q-learning settings\n","epsilon = 0.9  # Exploration rate\n","alpha = 0.2    # Learning rate\n","gamma = 0.9    # Discount factor\n","\n","# Game state\n","EMPTY = 0\n","X = 1\n","O = -1\n","\n","class TicTacToe:\n","    def __init__(self):\n","        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n","        self.current_player = X\n","\n","    def reset(self):\n","        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n","        self.current_player = X\n","\n","    def is_winner(self, player):\n","        for row in range(BOARD_ROWS):\n","            if np.all(self.board[row, :] == player):\n","                return True\n","        for col in range(BOARD_COLS):\n","            if np.all(self.board[:, col] == player):\n","                return True\n","        if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n","            return True\n","        return False\n","\n","    def is_full(self):\n","        return np.all(self.board != EMPTY)\n","\n","    def available_actions(self):\n","        return [(r, c) for r in range(BOARD_ROWS) for c in range(BOARD_COLS) if self.board[r, c] == EMPTY]\n","\n","    def make_move(self, row, col):\n","        if self.board[row, col] == EMPTY:\n","            self.board[row, col] = self.current_player\n","            self.current_player = O if self.current_player == X else X\n","            return True\n","        return False\n","\n","class QLearningAgent:\n","    def __init__(self):\n","        self.q_table = {}\n","\n","    def save_q_table(self, filename='q_table.pkl'):\n","        try:\n","            with open(filename, 'wb') as f:\n","                pickle.dump(self.q_table, f)  # Save Q-table to file\n","            print(f\"Q-table saved to {filename}\")  # Ensure save is successful\n","        except Exception as e:\n","            print(f\"Error saving Q-table: {e}\")\n","\n","    def load_q_table(self, filename='q_table.pkl'):\n","        try:\n","            with open(filename, 'rb') as f:\n","                self.q_table = pickle.load(f)  # Load Q-table from file\n","                print(\"Q-table loaded successfully.\")\n","        except FileNotFoundError:\n","            print(\"Q-table file not found, starting fresh.\")\n","            self.q_table = {}  # Initialize empty Q-table if file doesn't exist\n","        except Exception as e:\n","            print(f\"Error loading Q-table: {e}\")\n","\n","    def get_state_key(self, board):\n","        return str(board.reshape(9))\n","\n","    def choose_action(self, state, available_actions):\n","        if random.uniform(0, 1) < epsilon:\n","            return random.choice(available_actions)\n","        else:\n","            q_values = [self.q_table.get((state, (r, c)), 0) for r, c in available_actions]\n","            max_q = max(q_values)\n","            return available_actions[q_values.index(max_q)]\n","\n","    def learn(self, state, action, reward, next_state, available_actions):\n","        state_key = self.get_state_key(state)\n","        next_state_key = self.get_state_key(next_state)\n","        current_q = self.q_table.get((state_key, action), 0)\n","\n","        # Get maximum Q-value of the next state\n","        future_q = max([self.q_table.get((next_state_key, a), 0) for a in available_actions], default=0)\n","\n","        # Update the Q-value for the state-action pair\n","        self.q_table[(state_key, action)] = current_q + alpha * (reward + gamma * future_q - current_q)\n","\n","        #print(f\"Updated Q-value for {state_key}, {action}: {self.q_table[(state_key, action)]}\")\n","\n","def draw_board(board):\n","    # Draw grid lines\n","    for r in range(1, BOARD_ROWS):\n","        pygame.draw.line(screen, BLACK, (0, r * SQUARE_SIZE), (WIDTH, r * SQUARE_SIZE), LINE_WIDTH)\n","    for c in range(1, BOARD_COLS):\n","        pygame.draw.line(screen, BLACK, (c * SQUARE_SIZE, 0), (c * SQUARE_SIZE, HEIGHT), LINE_WIDTH)\n","\n","    # Draw X and O\n","    for r in range(BOARD_ROWS):\n","        for c in range(BOARD_COLS):\n","            if board[r, c] == X:\n","                pygame.draw.line(screen, GREEN, (c * SQUARE_SIZE + 10, r * SQUARE_SIZE + 10), (c * SQUARE_SIZE + SQUARE_SIZE - 10, r * SQUARE_SIZE + SQUARE_SIZE - 10), LINE_WIDTH)\n","                pygame.draw.line(screen, GREEN, (c * SQUARE_SIZE + SQUARE_SIZE - 10, r * SQUARE_SIZE + 10), (c * SQUARE_SIZE + 10, r * SQUARE_SIZE + SQUARE_SIZE - 10), LINE_WIDTH)\n","            elif board[r, c] == O:\n","                pygame.draw.circle(screen, RED, (c * SQUARE_SIZE + SQUARE_SIZE // 2, r * SQUARE_SIZE + SQUARE_SIZE // 2), SQUARE_SIZE // 3)\n","\n","# Initialize Pygame\n","pygame.init()\n","screen = pygame.display.set_mode((WIDTH, HEIGHT))\n","pygame.display.set_caption('Tic Tac Toe with Q-Learning')\n","\n","def main():\n","    game = TicTacToe()\n","    agent = QLearningAgent()\n","\n","    # Load Q-table\n","    agent.load_q_table()\n","\n","    clock = pygame.time.Clock()\n","    running = True\n","\n","    train_rounds = 3000  # Maximum number of training rounds\n","    current_round = 0  # Current training round\n","\n","    while running and current_round < train_rounds:\n","        screen.fill(WHITE)\n","        draw_board(game.board)\n","\n","        # Check if the game is over and print the result\n","        if game.is_winner(X):\n","            print(\"X wins!\")\n","            reward = -1  # Penalize AI if X wins\n","            game.reset()  # Reset the game\n","            current_round += 1  # Increase training rounds after game ends\n","        elif game.is_winner(O):\n","            print(\"O wins!\")\n","            reward = 1  # Reward AI if O wins\n","            game.reset()  # Reset the game\n","            current_round += 1  # Increase training rounds after game ends\n","        elif game.is_full():\n","            print(\"It's a draw!\")\n","            reward = 0  # Zero reward for a draw\n","            game.reset()  # Reset the game\n","            current_round += 1  # Increase training rounds after game ends\n","        else:\n","            reward = -0.1  # Small penalty if the game is ongoing\n","\n","        # AI makes a move if the game is still ongoing\n","        state = game.board.copy()\n","        available_actions = game.available_actions()\n","\n","        # If there are available actions, AI makes a choice\n","        if available_actions:\n","            action = agent.choose_action(agent.get_state_key(state), available_actions)\n","            game.make_move(action[0], action[1])\n","\n","            # Update Q-table at the end of each round\n","            next_state = game.board.copy()\n","            agent.learn(state, action, reward, next_state, available_actions)\n","\n","        pygame.display.flip()\n","        clock.tick(30)  # Control the frame rate of the game\n","\n","        # Exit after 3000 rounds\n","        if current_round >= train_rounds:\n","            print(\"Training finished after 3000 rounds.\")\n","            break\n","\n","    # Save Q-table after the game finishes\n","    agent.save_q_table()\n","\n","    pygame.quit()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"vwPLcX33QbUW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Code for the player to play against the agent\n","\n","Explanation:\n","This code file allows the player to play Tic Tac Toe against a trained Q-learning agent. The player can input their actions, and the agent responds based on the trained Q values. The code includes the following elements:\n","\n","*   Load the trained Q table.\n","*   Provide a user interface that allows the player to select actions and displays the current game state.\n","*   Implement the game win and loss logic and provide result feedback after the game is over."],"metadata":{"id":"y8zCbRN7QvFX"}},{"cell_type":"code","source":["import pygame\n","import numpy as np\n","import random\n","import pickle\n","\n","# Game settings\n","WIDTH, HEIGHT = 300, 300\n","LINE_WIDTH = 10\n","BOARD_ROWS, BOARD_COLS = 3, 3\n","SQUARE_SIZE = WIDTH // BOARD_COLS\n","\n","# Colors\n","WHITE = (255, 255, 255)\n","BLACK = (0, 0, 0)\n","RED = (255, 0, 0)\n","GREEN = (0, 255, 0)\n","BLUE = (0, 0, 255)\n","\n","# Q-learning settings\n","epsilon = 0.5  # Exploration rate\n","alpha = 0.5    # Learning rate\n","gamma = 0.9    # Discount factor\n","\n","# Game state\n","EMPTY = 0\n","X = 1\n","O = -1\n","\n","class TicTacToe:\n","    def __init__(self):\n","        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n","        self.current_player = X\n","\n","    def reset(self):\n","        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n","        self.current_player = X\n","\n","    def is_winner(self, player):\n","        for row in range(BOARD_ROWS):\n","            if np.all(self.board[row, :] == player):\n","                return True\n","        for col in range(BOARD_COLS):\n","            if np.all(self.board[:, col] == player):\n","                return True\n","        if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n","            return True\n","        return False\n","\n","    def is_full(self):\n","        return np.all(self.board != EMPTY)\n","\n","    def available_actions(self):\n","        return [(r, c) for r in range(BOARD_ROWS) for c in range(BOARD_COLS) if self.board[r, c] == EMPTY]\n","\n","    def make_move(self, row, col):\n","        if self.board[row, col] == EMPTY:\n","            self.board[row, col] = self.current_player\n","            self.current_player = O if self.current_player == X else X\n","            return True\n","        return False\n","\n","class QLearningAgent:\n","    def __init__(self):\n","        self.q_table = {}\n","\n","    def save_q_table(self, filename='q_table.pkl'):\n","        with open(filename, 'wb') as f:\n","            pickle.dump(self.q_table, f)  # Save Q-table to file\n","\n","    def load_q_table(self, filename='q_table.pkl'):\n","        try:\n","            with open(filename, 'rb') as f:\n","                self.q_table = pickle.load(f)  # Load Q-table from file\n","                print(\"Q-table loaded successfully.\")\n","        except FileNotFoundError:\n","            print(\"Q-table file not found, starting fresh.\")\n","            self.q_table = {}  # Initialize Q-table if file not found\n","\n","    def get_state_key(self, board):\n","        return str(board.reshape(9))\n","\n","    def choose_action(self, state, available_actions):\n","        if random.uniform(0, 1) < epsilon:\n","            return random.choice(available_actions)\n","        else:\n","            q_values = [self.q_table.get((state, (r, c)), 0) for r, c in available_actions]\n","            max_q = max(q_values)\n","            return available_actions[q_values.index(max_q)]\n","\n","    def learn(self, state, action, reward, next_state, available_actions):\n","        state_key = self.get_state_key(state)\n","        next_state_key = self.get_state_key(next_state)\n","        current_q = self.q_table.get((state_key, action), 0)\n","\n","        # Get maximum Q-value of the next state\n","        future_q = max([self.q_table.get((next_state_key, a), 0) for a in available_actions], default=0)\n","\n","        # Update the Q-value for the state-action pair\n","        self.q_table[(state_key, action)] = current_q + alpha * (reward + gamma * future_q - current_q)\n","\n","def draw_board(board):\n","    # Draw grid lines\n","    for r in range(1, BOARD_ROWS):\n","        pygame.draw.line(screen, BLACK, (0, r * SQUARE_SIZE), (WIDTH, r * SQUARE_SIZE), LINE_WIDTH)\n","    for c in range(1, BOARD_COLS):\n","        pygame.draw.line(screen, BLACK, (c * SQUARE_SIZE, 0), (c * SQUARE_SIZE, HEIGHT), LINE_WIDTH)\n","\n","    # Draw X and O\n","    for r in range(BOARD_ROWS):\n","        for c in range(BOARD_COLS):\n","            if board[r, c] == X:\n","                pygame.draw.line(screen, GREEN, (c * SQUARE_SIZE + 10, r * SQUARE_SIZE + 10), (c * SQUARE_SIZE + SQUARE_SIZE - 10, r * SQUARE_SIZE + SQUARE_SIZE - 10), LINE_WIDTH)\n","                pygame.draw.line(screen, GREEN, (c * SQUARE_SIZE + SQUARE_SIZE - 10, r * SQUARE_SIZE + 10), (c * SQUARE_SIZE + 10, r * SQUARE_SIZE + SQUARE_SIZE - 10), LINE_WIDTH)\n","            elif board[r, c] == O:\n","                pygame.draw.circle(screen, RED, (c * SQUARE_SIZE + SQUARE_SIZE // 2, r * SQUARE_SIZE + SQUARE_SIZE // 2), SQUARE_SIZE // 3)\n","\n","def draw_text(text, y_offset, size, color, background_color=None):\n","    font = pygame.font.Font(None, size)\n","    text_surface = font.render(text, True, color, background_color)\n","    text_rect = text_surface.get_rect(center=(WIDTH // 2, y_offset))\n","    screen.blit(text_surface, text_rect)\n","\n","def draw_buttons():\n","    pygame.draw.rect(screen, BLUE, (100, 200, 100, 30))   # Reset button\n","    pygame.draw.rect(screen, BLUE, (100, 230, 100, 30))  # Quit button\n","    draw_text(\"Reset\", 215, 30, WHITE)  # Set text position for Reset, y-coordinate just below the button\n","    draw_text(\"Quit\", 245, 30, WHITE)     # Set text position for Quit, y-coordinate just below the button\n","\n","# Initialize Pygame\n","pygame.init()\n","screen = pygame.display.set_mode((WIDTH, HEIGHT))\n","pygame.display.set_caption('Tic Tac Toe with Q-Learning')\n","\n","def main():\n","    game = TicTacToe()\n","    agent = QLearningAgent()\n","    agent.load_q_table()\n","    clock = pygame.time.Clock()\n","    running = True\n","    game_over = False\n","    result_message = \"\"\n","\n","    while running:\n","        screen.fill(WHITE)\n","        draw_board(game.board)\n","\n","        if game_over:\n","            draw_text(result_message, 100, 30, WHITE, BLUE)\n","            draw_buttons()\n","\n","            for event in pygame.event.get():\n","                if event.type == pygame.QUIT:\n","                    running = False\n","                if event.type == pygame.MOUSEBUTTONDOWN:\n","                    mouseX, mouseY = event.pos\n","\n","                    if 100 <= mouseX <= 200 and 200 <= mouseY <= 230:\n","                        # Reset button\n","                        game.reset()\n","                        game_over = False\n","                    elif 100 <= mouseX <= 200 and 230 <= mouseY <= 260:\n","                        # Quit button\n","                        running = False\n","            pygame.display.flip()\n","            clock.tick(30)  # Control the frame rate of the game\n","            continue  # Skip the rest of the code and return to the loop\n","\n","        # Check if the game is over\n","        if game.is_winner(X):\n","            result_message = \"X wins!\"\n","            game_over = True\n","            reward = -1  # Penalize AI if X wins\n","        elif game.is_winner(O):\n","            result_message = \"O wins!\"\n","            game_over = True\n","            reward = 1  # Reward AI if O wins\n","        elif game.is_full():\n","            result_message = \"It's a draw!\"\n","            game_over = True\n","            reward = 0  # Zero reward for a draw\n","        else:\n","            reward = -0.1  # Small penalty if the game is ongoing\n","\n","            for event in pygame.event.get():\n","                if event.type == pygame.QUIT:\n","                    running = False\n","                if event.type == pygame.MOUSEBUTTONDOWN:\n","                    mouseX, mouseY = event.pos\n","                    row = mouseY // SQUARE_SIZE\n","                    col = mouseX // SQUARE_SIZE\n","\n","                    # Player's move\n","                    if game.make_move(row, col):\n","                        state = game.board.copy()\n","                        available_actions = game.available_actions()\n","\n","                        # Let AI make a move after checking if the game is over\n","                        if not game.is_winner(X) and not game.is_winner(O) and available_actions:\n","                            action = agent.choose_action(agent.get_state_key(state), available_actions)\n","                            game.make_move(action[0], action[1])\n","\n","                            # Update Q-table at the end of each round\n","                            next_state = game.board.copy()\n","                            agent.learn(state, action, reward, next_state, available_actions)\n","\n","        pygame.display.flip()\n","        clock.tick(30)  # Control the frame rate of the game\n","\n","    # Save Q-table after the game ends\n","    agent.save_q_table()\n","\n","    pygame.quit()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"Xia8XUdnQ81T"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3-azureml","language":"python","display_name":"Python 3.6 - AzureML"},"language_info":{"name":"python","version":"3.6.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernel_info":{"name":"python3-azureml"},"nteract":{"version":"nteract-front-end@1.0.0"},"microsoft":{"host":{"AzureML":{"notebookHasBeenCompleted":true}}},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}